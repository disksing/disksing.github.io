<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>分布式系统 on 硬盘在歌唱</title><link>http://disksing.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</link><description>Recent content in 分布式系统 on 硬盘在歌唱</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 11 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="http://disksing.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/index.xml" rel="self" type="application/rss+xml"/><item><title>双中心主从模式</title><link>http://disksing.com/dual-datacenter-master-slave/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>http://disksing.com/dual-datacenter-master-slave/</guid><description>在之前的Paxos从入门到学会Raft一文中，为了引入paxos/raft共识算法，简单地讨论了一下主从模式以及为什么主从模式不能最大限度地同时保证高可用和一致性。不过在现实场景中，常常因为基础设施不足，网络成本控制等原因，无法使用需要三中心的paxos/raft，所以我想再详细讨论下只有两中心的场景下的妥协方案。
简单回顾下上次的讨论：在两中心主从模式下，我们如果想要主从切换时不丢数据，就必须使用同步模式，即主中心写入的数据，需要同步到从中心落盘，再给客户端返回写入成功。
同步模式的困境在于，从任何一个中心的视角来看，都无法区分出“另一个中心故障”和“两中心网络断连”这两种异常情况。这导致主从模式下的failover是一定无法由程序自动进行的：
如果主中心在发现从中心故障（或断连）时自动切换至独立运行（异步）模式，那么当主中心发生故障时，从中心无法切换至独立运行模式，因为从中心无从判断主中心是真故障了，还是网络断连了（这种情况下主中心可能已切换至独立运行模式）。
反过来如果我们让从中心在发现主中心故障（或断连）时自动切换至独立运行模式，同样的道理，我们也无法处理从中心故障的情况。
这里我们面对的是一个经典的CA抉择问题，大方向上有两个选择。
第一个方向是优先保证一致性，一旦出现故障或者网络断连了就主动停止服务，由运维来选择一个中心来恢复服务。我们都知道，一旦需要人工介入，高可用性这块基本上就免谈了。还有一个可能产生麻烦的点在于在发生故障的前提下（且可能是网络故障），或许会给运维人员接入生产环境带来一定的困难，这也会进一步增加故障恢复时间。
另一个方向就是优先保证高可用。典型的做法是主中心在发现从中心故障（或断连）时，自动切换成异步模式提供服务。如前所述，一旦主中心故障，就需要人工干预来进行主备切换了。要特别注意这种情况下主备切换不仅比较tricky还有丢数据（不一致）的可能，以下两段划重点：
主备切换之前，需要先把主中心的服务给“掐掉”，因为主中心是有自动切换异步模式单独服务的机制的，如果在主备切换之后假性故障的主中心死灰复燃，两个中心同时提供服务，会产生非常严重的后果！那么怎么把主中心的服务掐掉呢？如果此时能接入主中心，可以直接停掉所有进程或者通过配置开关禁用自动切异步模式的功能；还可以通过网关、防火墙等配置断开应用和主中心数据服务的连接；另一个选项是在应用层做主备切换，保证所有应用都只连接从中心的服务。如果这些都做不到，此时做主备切换就要承担很大的风险了，建议上报给老板做决策。
主从数据可能不同步。理论上讲在同步模式下主中心发生故障，从中心一定有全量数据。但是实际上如果此时不能接入主中心检查状态，我们单看从中心无法排除这种可能性：主中心在完全故障之前，先跟从中心发生网络断连，随后自动切换异步模式并写入了一些数据。因此，做主备切换之前，除非能接入主中心并确认其没有切换过异步模式，还要承担丢数据的风险，这里同样建议您先上报老板。
工程实践中，C和A并不是非此即彼的选择题，常常有权衡的空间。在双中心主备切换的场景中，我们可以牺牲一些可用性来换一些一致性。
做法也很简单，就是给主中心切换异步模式设置一个比较大的超时时间，比如30分钟。这样当从中心故障时，主中心需要等待30分钟才能独立提供服务，牺牲了可用性。换来的一致性保证是，当主中心故障（或断连）时，如果我们在30分钟之内做主备切换，就能确定一定不丢数据。
具体的超时时间可以根据需要进行调整，很大的值其实就是一致性模式，很小的值对应的是高可用模式。看到这里熟悉Oracle的朋友们应该都会心一笑了，这其实就是DataGuard的maximize protection模式和maximize availability模式嘛。
接下来简单聊一下架构层面改进的两种思路。
第一个思路是向三中心架构推进一步。开头也说了使用paxos/raft三中心的主要问题是成本太高了，然而实际上不需要完整的三中心三副本也能达到比较好的效果。主备模式的主要缺陷在于两个数据中心的地位是完全对等的，出现网络隔离的时候无法判断对面是不是真挂了。
我们可以在第三个数据中心部署一个简单的etcd服务来扮演“仲裁者”的角色，在主从中心正常工作的时候，不需要与etcd作任何消息交换，发生故障时，哪边能连上etcd，哪边就有权切换成独立服务模式。假如主从网络断连，同时它们又同时能连接到etcd，那么主中心切换成异步模式并把状态信息通过etcd传递到从中心。
由于跟第三个数据中心之间只需要传递简单的状态信息，可以考虑使用移动网络、无线电或者卫星通信（如北斗短报文）来进一步减少成本。
另一个思路成本更低一些，不使用第三个数据中心，直接在主从中心之间建立低成本旁路通信同步状态。这时因为没有“仲裁者”了，在出现整个数据中心故障时仍然会陷入“两难”的境地，不过在发生网络故障时，主从中心可以通过旁路进行状态交换然后迅速进行异步模式切换。
最后来点主题升华哈。权衡是软件工程架构设计的最经典命题之一，绝大部分情况下都没有完美的设计，只有特定约束条件下最平衡的设计。诚然，一个又一个“不可能三角”告诉我们完美的系统是不存在的，但换个角度来看，不可逾越的鸿沟也是可以去不断逼近的极限，这就是工程师的浪漫呀！</description></item><item><title>TrueTime和原子钟</title><link>http://disksing.com/truetime/</link><pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate><guid>http://disksing.com/truetime/</guid><description>如果你关注分布式数据库，相信多少听说过Google的分布式数据库Spanner，以及Spanner使用原子钟搞了一套TrueTime来实现跨数据中心的分布式事务。
而Spanner的后继者们，却都采用了不使用原子钟替代方案，比如TiDB的TSO，CockroachDB的HLC。对此很多人的印象就是Google财大气粗，所以有能力搞原子钟这种精密高端设备。这个说法不能说全错，但至少不是完全准确的。
网络时钟同步 上面提到的3种取时间戳的方式的底层逻辑是迥然不同的。TiDB的TSO是中心授时，每一个时间戳都要从中心服务器获取；CockroachDB的HLC本质上是逻辑时钟，依赖于消息交换时去推进时钟计数器；Spanner的TrueTime是时钟同步，通过定期交换消息，把本地时钟与源时钟进行同步。
时钟同步的模式跟我们日常用手表的方法是类似的，我们隔一段时间把手表跟新闻联播同步一下，期间的时间直接从手表上读出来。
计算机里面最常见的时钟同步就是NTP了，通过网络同步时钟有个问题就是延迟导致的误差。
网络同步时钟比如客户端在12:00:00发起查询请求，2秒钟后收到服务器的消息，返回的时间也是12:00:00。这时并不意味着本地时钟是准确的，因为消息发到服务器要花费时间，本地时钟实际上是快了一点。但是具体快了多少是没法知道的，我们只知道消息一来一回花了2秒，却不知道来回分别花了多长时间。因此只能大概估摸着取个中间值，把时间往回拨1秒，这时误差范围就是±1秒了。
Marzullo算法 只从单一时间源同步时间是不够靠谱的。除了有可能发生故障或者网络中断，更可怕的是时间源本身就出了问题。Marzullo算法就是用来从多个时间源来估算准确时间的算法。
Marzullo算法如图，我们通过向ABCD四个时间源查询时间分别得到时钟偏差及误差范围，算法的大体思路就是选被尽可能多时间源所覆盖的区间（缩小误差范围），并排除掉有问题的区间（如A）。
不过，在对时序有严格要求的场景（比如分布式事务），Marzullo算法还要进行一些改良。例如比较明显的缺陷是，当有问题的时间源offset区间与正常的区间有交叠时，可能导致误差范围被估算得过小。如果想了解相关细节，可以去研究下相关资料，这里不展开了。
时钟漂移 跟服务器对上时间了还没完，通常对时的过程都要周期性地触发。正如我们的手表用着用着就不准了，CPU的晶振周期也不是完全精确的，会受温度和电压的影响，时间一长也会“跑偏”。
Spanner假设他家服务器的误差不超过每秒钟200μs。按最大值去计算，30秒不同步，误差最多会累计到6ms，如果1天不同步，最大误差达到约为17s。要注意这里的误差范围是非常非常保守的，实际情况CPU远不可能这么糟糕，举个例子对比一下，我国石英电子表的行业标准是，一类月差10-15秒，二类月差20-30秒。
原子钟 原子钟，是一种利用原子、分子能级差为基准信号来校准晶体振荡器或激光器频率，以使其输出标准频率信号的一种装置。它的工作原理是：利用原子吸收或释放能量时发出的电磁波来计时的。由于这种电磁波非常稳定，再加上利用一系列精密的仪器进行控制，原子钟的计时就可以非常准确了，可以达到千万年仅差一秒或者更好的水平。
—— 时间频率：5G 叠加自主可控， 被忽视的高精尖领域
看上去确实很高端，那么假如想买这样一个原子钟要多少钱呢？实际情况是原子钟比听上去亲民的多，我们直接在东哥的网站上就能搜到：
京东商城售卖的原子钟售价大约是几万到十几万不等，并非承受不起的昂贵，和一台高端点的服务器是差不多的价位，如果降低精度的要求还能更便宜。
说白了原子钟和计算机上面随处可见的晶振就是同一类东西，只不过精度高了好几个数量级。
不同硬件的计时精确度需要注意有些同学误认为TrueTime需要每台机器都要给配一个原子钟，其实不用，一个数据中心有几个就完全足够了，具体先按下不表后面再说。
GPS授时 GPS不仅提供定位服务，还可以授时。每个GPS卫星都携带了数个高精度原子钟，并不断广播星历（运行轨迹）和时间。地面装置从至少4颗卫星接收到信号后，解开以三维空间+一维时间为变量的四元方程组，就能同时拿到时间空间信息了。
GPS的精度非常之高，可以把误差控制在数纳秒以内。这是因为电磁波信号基本上是直线传播，路径上受到的干扰很小，根据距离可以很准确地计算出信号传递延时。而网络消息会受中继和多层网络层层封包的影响，而且即便在光纤中，信号也不是沿直线传播的。
TrueTime 背景知识介绍完毕，下面我们就来看看TrueTime到底是怎么做的。
机房内的TrueTime组件部署TrueTime组件按角色分成 time master 和 time daemon。time master 可以认为是 TrueTime 的服务端，部署在一些独立的机器上，time daemon 是客户端，以进程的形式部署在每个实际运行业务的主机上。
time master 又分成两类。一类安装 GPS 模块，分散在机房的不同位置，每个GPS节点都使用独立的天线，避免因为信号干扰的原因一起失效了。另一类安装的是原子钟，原子钟也是多台来防止故障产生不可用。
各种 time master 周期性地使用Marzullo算法相互对时，每个 time daemon 也会以 30 秒为周期跟多个 time master 进行对时（同样使用Marzullo算法）。
之前介绍过，GPS 的精度是纳秒级别的，这个误差跟机房内的网络延迟比起来都可以忽略不计了，直接计作0ms。这样time daemon进行时钟同步之后的误差就仅仅取决于网络延迟了，一般机房内不超过1ms。
我们还要考虑到完成时钟同步之后，到下一次同步期间time daemon的时钟漂移，也就是前面计算过的，30秒内最大误差可能累计到6ms。于是，time daemon上的时钟误差范围就在1ms到7ms之间不断涨落，画出来是这样的锯齿状：
time daemon误差范围变化示意那么问题来了，原子钟是干啥用的？</description></item><item><title>Paxos从入门到学会Raft</title><link>http://disksing.com/paxos/</link><pubDate>Tue, 20 Oct 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/paxos/</guid><description>我觉得学习Paxos/Raft的最大障碍并不是算法本身复杂，而是难以理解。就好像某些数学结论，证明过程不难，但是结论却很难从直观上去理解。本文就是希望能借助一个假想中的系统，逐步加强约束，引导到Paxos/Raft，希望能一定程度上解释“为啥要用共识算法”以及“不用共识算法会怎样”的问题。
本文结构在很大程度上参考了drdrxp阁下的一个PPT，他的微博主页上也有对应一篇很棒的关于Paxos的文章，这里表示感谢及一并推荐给大家。不过因为理解角度不同，本文很多地方有诸多差异，例如关于半同步复制为什么不可行，本文给出了另一种解释，另外这里没有讲Fast Paxos，但是多了关于Raft的内容，希望读者可以进行比较阅读：）
单机 我们假想一个抢购手机的网络服务，因为这款手机的用户都比较发烧，所以一次只卖一个手机。在活动之前，系统会给每个用户分配一个E码作为唯一标识，抢购时间到达之后，所有用户通过客户端发送E码到服务器，服务器把手机分配给一个用户。
第一版设计我们使用单机服务器模式：搞一台主机作为服务器，当收到第一个请求后，保存这个用户的E码，并给客户端返回“抢购成功”，对于后续的所有请求，只要E码跟保存的不一样，一律返回“抢购失败”。
单机模式的缺陷大家都耳熟能详了，就是不能容忍节点发生故障。仅有的一台服务一旦故障，整个服务就不能用了，这个指的是可用性。还有一个批判的角度是容灾性，如果这台服务器的数据损坏了，我们将无从判断这台手机是否已经被卖给了某个用户。
备份（异步复制） 大家都知道用户数据是非常重要的资产，万万不能丢，一定要备份。
所谓备份，就是定期把数据拷贝一份放在别的地方。还有一个概念叫异步复制，其实本质上差别不大，我们放在一起讨论。这里说异步，指的是最新的数据并不是与备份副本实时同步的。
备份能解决一部分数据容灾的问题。这里限定说“一部分”，是因为异步模式存在一个不同步的时间窗口。如果Master在(3)OK返回给客户端之后故障了，E的值将不能被复制到Slave。之后如果使用Slave数据来恢复服务，手机将再次被卖给另外一个人，也就是一致性被破坏了。
同步复制 异步的不行，那同步的怎么样呢？
如图所示，Master收到请求后，先同步给Slave，Slave存盘后返回OK，然后Master再存盘并给客户端返回OK。
如果Slave故障了，我们把Master切换成单机模式继续提供服务。如果Master故障了，我们就把Slave切换成Master提供服务。因为是同步的，两种情况都不会产生数据丢失。
注意这里假设Slave在存完盘返回消息之前故障，也不算丢数据，因为此时Master并没有给客户端返回OK，所以手机是可以再卖给另一个人的，只需要在Slave恢复之后，Master再把新值同步过去就行了。
看上去就很完美了，可用性和一致性都能得到保证，只需要有一个负责任的工程师来盯着服务器，故障的时候切一下状态就行了。
问题就出在这个工程师身上。
我们必须要把工程师这个人也算成分布式系统的一部分，要考虑到人也会故障的（生病，意外，手机欠费失联，突然想去看看世界），而且通常管理员也是通过网络来运维管理，当服务器节点之前网络中断时，管理员也很可能无法访问某些节点。实际上我们完全可以把工程师看作集群里的一个故障检测程序来分析问题。
如图(A)，假如Admin节点离Master比较近，那么当他们一起故障时，Slave无法被提升成Master。同理(图B)，Admin跟Slave一起故障时，Master也无法切换成单机模式。
那我多搞几个Admin，分别跟Master/Slave部署在一起行不行？也是不行的，这样看起来Master/Slave不管谁故障了，另一个没故障的总有Admin来操作。但是假如发生了网络隔离，如果Admin判断对面故障了，贸然切换状态，可能会出现两都是Master同时提供服务，一致性被破坏。
还有一种打补丁的思路，就是引入一个仲裁者(图D)的角色，Master和Slave不断心跳上报状态，发现对面失联想切换状态时，也要向Meta申请。这样一来，当Master和Slave断开时，取决于谁跟Meta是连着的，以及谁能更快地把状态切换请求发给Meta。
不过这里的问题在于，如何保证Meta的高可用和容灾性呢？（禁止套娃）
半同步复制 回顾一下上面提到的各种方案，我们能发现一个有趣的现象：每次都是跪在系统中的特殊节点上面。比如仲裁者Meta，或者负责切换状态的Admin，还可以包括单机模式下的那个唯一单点。由于特殊节点的不可替代性，一旦故障了，牵一发动全身，整个系统就离挂掉不远了。
说明一下，这里从可用性来分析，我们不认为Master是特殊节点，因为Master和Slave是可以相互替代的。
从消除特殊节点的思路出发，我们把之前方案里的仲裁者Meta换成Slave，就得到了半同步复制模式。
具体来说，Master收到消息先本地持久化，然后同时同步给两个Slave，当其中任意一个Slave完成持久化并返回OK后，Master返回OK给客户端。
不难分析，任意一个Slave故障时，都不会影响服务。假如Master故障，则需要两个Slave挑一个出来当新的Master，此时可能只有一个Slave同步到数据，我们需要选择有数据的节点当Master。如果两个Slave都没数据，那任选一个就行。
这里的Slave其实同时承载了“仲裁节点”的角色，当Master和另一个Slave断连时，如果此Slave能连上Master，则支持Master继续提供服务，反之如果此Slave只能连到另一个Slave，那这两个Slave放弃旧Master选个新的出来。
如此这般，这个方案能很好地满足单节点故障时的可用性和一致性，而且规则简单，不需要人工介入就能自动完成。可惜它还是有缺陷的，前面我们其实只分析了单次故障的情形，如果连续多次故障，就不行了。
如图，Master本地写完E=1后故障了，Slave选出新的Master然后写入E=2，随后新Master也故障同时旧Master又活过来了，然后剩下的两个节点都有数据，还都不一样，你瞧瞧我，我瞧瞧你，不知道谁来当Master合适。
你可能想说，我们改下流程，写入时先在Slave持久化，OK返回给Master后再在Master持久化，这样是不是就行了？这样也是不行的，因为Slave可能在刚持久化之后就故障了，随后另外两个节点写入新值并再次故障，最后结果是一样的。
半同步复制还可以进一步打补丁，不过这里我们先放一放，来看一下另一个思路。
多写 如果我们进一步消除节点的特殊性，即不再区分Master和Slave，可以得到另一个方案：客户端把请求同时发向3个节点，当其中2个节点返回OK后，就认为写入成功。
如图所示，Node1和Node2成功持久化了E=1并返回OK，之后Client2再尝试写入E=2时，最多只能写入Node3一个节点，因此无法成功写入，这样我们就保证了手机不可能被卖给2个人。
这里我们利用了“鸽巢原理”：client1和client2要想都写入成功，需要各收到2个OK，而每个节点都只会给第一个请求的客户端发送OK，也就是说总共只能发出去3个OK，因此只有一个客户端能写入成功。
这个规律也可以推广至更多数量的节点，只要规定要求写入的节点数大于一半，就只能写成功一个。
还有一种表述是，两个包含大多数成员的子集，一定至少有一个公共节点。这个性质十分重要，后面我们还会用到。
这个方案的问题在于，它能保证手机不被卖给多个人，但是保证不了手机一定能卖出去。比如3个节点收到的第一个请求分别来自不同的客户端，此时任何一个客户端都无法收集到足够数量的OK。
此外的矛盾之处在于：一方面，节点应该避免先后被多次写入来确保手机不被卖给多人；另一方面，节点又需要能“擦除”已经写入的数据来使得手机最终一定能被卖出。
不难发现，能被安全擦除的值，一定是没有成功写入大多数节点的，一旦写入了大多数节点，客户端就认为写入成功，如果再允许其他客户端写入成功，手机也就被卖给多个人了。
在多写模式下，不存在Master那样的特殊节点，最后手机卖给谁了，不取决于某一个节点，而是由集群中的大多数节点决定。
WRN 多写模式下应该如何去读取数据，DynamoDB和Cassandra所用的WRN模型给出了一个思路。所谓WRN，是指有N个节点的集群，写入时同时写入W个节点，读取时查询R个节点，当保证W+R&amp;gt;N时，同样根据“鸽巢原理”，我们能知道W和R一定至少有一个公共节点，因此先写入的值一定会被后面的读取“看到”。
大家都知道，DynamoDB和Cassandra都是最终一致性的。它们的弱一致性，主要体现在写入进行的过程中进行多次读取，可能有时能读到写入的数据，有时又读不到，根据读取所查询的节点不同而得到不同的结果。
此外，写入成功的值一定会被读到，不意味着读到的值一定写入成功或将要写入成功。假设客户端只写入了一个节点就故障了，数据仍然可能被其他客户端读取到。
WRN还给了我们一点提示，想要集群节点的两个子集有公共节点，不一定要取两个大多数节点，只需要加起一起数量大于N就行了。从高可用的角度来看，W和R分别取刚好超过一半节点通常是一个好选择，因为这样可以容忍最多不超过一半的节点故障。当然了，假如业务只关心写入请求的高可用，完全可以让W=1,R=N，此时只要连上一个节点就能写入，但是不同节点可能写入不同的值，需要在读的时候处理冲突，这就是典型的CAP理论中牺牲C来换取A了。
多读+多写 基于此我们有了改进思路：服务器端总是允许用新值覆盖旧值；客户端使用一种两阶段的流程，在写入之前先进行一轮读取，如果发现已经有值被写入了大多数节点，就说明手机已经被卖出去了，否则可以尝试写入新值。
很显然，与WRN类似，这个方案也有并发问题。当client2发起读取时，client1的写入还没有开始或者进行到一半，此时client2认为没有旧值被成功写入，于是发起写入，而在client2写入成功之前，client1也写入成功了，这样，手机又被卖给了两个人。
这个方案不能成功的原因是，第一阶段的读取的结果不能保持到第二阶段的写入，写入请求到达服务器时，前置条件已经不成立了。
一种可能的改进方法是使用某种锁机制，第一阶段读取时，把读过的节点上锁，第二阶段写入时再解锁。只是这么做的副作用也很显然，一旦上完锁之后客户端崩溃，或者与某些节点的网络断开，某些节点将没有机会被解锁。
我们要做的是把这个锁换成一种“活锁”。
Basic Paxos 在现实生活中有一个活锁的例子，就是拍卖。拍卖的时候，报价是不断上涨的，每当竞拍人给出一个报价时，之前所有更低的报价就失效了，同时产生了一个交易确认窗口期，如果没有人出更高报价，交易就会被确认。
Paxos的工作方式是类似的。每个客户端可以不断生成递增且互不重复的proposal id，写入分为读写两阶段，分别叫_prepare_和_accept_，如果两个阶段之间没有被更大的proposal id打断，写入就能成功。
Paxos把我们之前描述的抢手机的问题抽象为“多个节点共同确认一个值”的问题，把我们的服务器节点叫acceptor，客户端叫proposer，当一个proposer把值写入超过半数的acceptor后，这个值就被确认了。
Paxos的工作过程是，在读取阶段，需要写入数据的proposer向所有acceptor发送自己的proposal id，acceptor保证一旦返回自己的状态，便不再接受proposal id更小的请求了。
我们尝试站在proposer的视角，来推断其收到大多数acceptor回复后，可能遇到的3种情况：
这些节点都没有value，说明此时没有value被确定，而且将来也不会有value被更小的proposal id确定（理由是大多数acceptor已经不再接受proposal id更小的请求了）。此时该proposer可以尝试发送accept消息来写入新值。 这些节点都返回了相同的value和proposal id，说明此时value已经被确定了。此时该proposer应该拒绝掉待写入的新值。 只有部分结果有value，或者这些节点返回的proposal id不完全一样。此时不确定是否有value已经或即将被更小的proposal id所确认，该proposer也不能写入新值。不过，能确定的是，如果已经有value已经或即将被提交，那么该value一定是所有acceptor返回的消息中proposal id最大的那一个（原因参考情况1，某个proposer写入了该value，意味着更小的proposal id都不可能成功）。此时为了得到确定的值，我们只能选择发送accept消息写入旧值。 在第二阶段，proposer把待写入的新值或旧值放在accept消息中发给所有的acceptor，再一次，当收到大多acceptor的返回消息后，该值就被确定了。如果在两个阶段之间插入了proposal id更大的prepare消息，写入将不会成功。这时proposer需要选择更大的proposal id并再次尝试两阶段写入。</description></item><item><title>分布式事务的 Commit Point</title><link>http://disksing.com/txn-commit-point/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/txn-commit-point/</guid><description>要说这个数据库事务啊，讲究的是 ACID。在分布式场景下，这四个没有一个是简单的，今天我们的话题主要涉及到 A(tomic)。
一、分布式环境的复杂性 在单机环境下，实现事务原子性并不复杂。一般的做法是事务提交之前的写入被存放在 预写式日志 中，然后在事务提交时，往磁盘追加一条 提交记录，完成事务的提交。
所谓 Commit Point，在这个场景下指的是 提交记录 被持久化到磁盘的一瞬间。在此之前，整个事务的写入都是未生效的状态，事务提交可能被回滚或中止（即使客户端已经发送了 Commit 命令，数据库可能在 Commit Point 之前崩溃）；而在 Commit Point 之后，整个事务就被提交成功了（即使由于数据库崩溃没来得及把结果返回给客户端）。
本质上，Commit Point 通过把事务内的多条 SQL 语句或者说多个对象的更新是否被提交“归约”到一个单点，也就是事务的 提交记录，从而确保了“要么同时提交，要么同时回滚”。
在分布式事务中，一个事务会同时牵扯到多个节点。这可能是因为事务本身要更新保存在不同节点的多个对象，也可能因为数据和索引保存在不同的节点（Global Index）。如果沿用单机数据库的经验，通过存储引擎中的 提交记录 来照葫芦画瓢，很容易出现原子性被破坏的情况：
部分节点成功提交，而部分节点由于冲突等原因需要回滚 部分节点成功提交，部分节点由于网络中断或崩溃无法提交 客户端与部分节点网络中断 客户端在向部分节点发送请求后崩溃 二、两阶段提交 两阶段提交（2PC）引入了 协调者（coordinator） 的角色，它通常以库的形式嵌入在发起事务的进程中，也可以以单独的进程或服务存在——这种情况下通常被称为 事务管理器（transaction manager）。同时我们把持有数据的存储节点称为 参与者（participant）。
两阶段提交（来源：设计数据密集型应用）所谓两阶段，是 协调者 在提交的过程中，分两个步骤分别与 参与者 交互：
发送 准备（prepare） 请求给所有 参与者 ，询问是否可能提交。 如果所有 参与者 都回复YES，则发起第 2 阶段的 提交（commit） 真正提交；如果任意一个 参与者 回复NO或者超时无响应，则第 2 阶段改为 中止（abort） 回滚之前的操作。 这个过程类似西方婚礼时的流程。神父在第一阶段询问新娘和新郎是否要结婚，如果新娘和新郎都回复YES，神父才进入第二阶段，宣布二人结为夫妻。如果任意一个人说NO，结婚就中止了。
很显然，这个流程能防止“部分节点由于冲突等原因需要回滚”，但是并不能防住由于崩溃或者网络中断导致的不同步。好比神父在宣布二人结为夫妻时，新郎由于心情激动晕倒了没有听到，他醒来以后结婚还是未完成的状态，原子性就这样被破坏了。
解决这种“不同步”的关键正是 Commit Point，正如前面所说，我们需要把多个对象的更新归约到一个单点。实践中，Commit Point 可以有多种选择，下面我们来逐一分析：</description></item><item><title>估算两台服务器同时故障的概率</title><link>http://disksing.com/failure-probability-analysis/</link><pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/failure-probability-analysis/</guid><description>在TiKV 的多副本机制一文中，我们讨论了 TiKV 集群可以容忍单节点故障，但是如果发生了多个节点同时故障，就可能导致不可用乃至于丢数据了，这个其实也适用于大多数依赖于多副本冗余数据的系统。因此，估算集群中多个服务器同时故障的概率是很有现实意义的，这个问题最简化的形式就是两台服务器同时故障，本文准备做这种情况的概率估算，然后用随机实验的方式加以验证。
概率模型估算 回忆当年《概率与统计》课程，依稀记得说电灯泡的损坏是服从于指数分布的，其分布函数是这样：
$$ P(t) = 1 - e^{-\lambda t} $$
这个公式可以用来计算灯泡寿命小于 t 的概率，其中$\lambda$代表损坏事件发生的频率，即单位时间损坏事件发生的次数。由于指数分布的“无记忆性”，也就是说假设每时每刻发生故障的概率是一样的，这个公式同样也可以用来计算两次故障的间隔。
说点题外小知识点，现实中产品（特别是电子产品）往往是有记忆性的，生命周期的不同阶段发生故障的概率并不完全相同，其规律表现为一条两侧陡峭，中间平坦的曲线，也被称为浴缸曲线。这也符合我们日常生活中的体验，新设备到手有一段“磨合期”，用时间长了有一段“老化期”，而这二者中间是相对稳定的。当然了本文不会考虑这么细，还是用简化模型来进行分析。
浴缸曲线示意图为了套用指数分布的公式，我们需要知道服务器的年故障率$\lambda$，这个可以在已有集群上统计出来，也可用参考 Google，微软等云服务厂商给出的相关报告，还可以通过硬件厂商给的 MTBF（平均故障间隔）来进行推算（把时间单位换算成年再求倒数）。
我们假设集群有固定的$N$台服务器，且每台服务器的年故障率都是$\lambda$，如果把集群当作一个整体，其故障的频率就是$N\lambda$，套用指数分布公式，这个集群在时间 t 内发生故障的概率是：
$$ P(Failure(t)&amp;gt;0)=1-e^{-N\lambda t} \tag{1} $$
接下来我们来计算两节点同时故障的概率。要注意此概率与“集群发生两次故障”的概率是不一样的，多了一个“同时”的约束。要知道故障都是一瞬间发生的，那是不是意味着两节点同时故障的概率就是 0 呢？其实也不是。在多副本冗余的场景下，一个节点故障以后，集群需要一个过程来消除这次故障所带来的影响（比如另找服务器补副本），如果下一次故障时影响已经完全消除了，那么数据安全是不受影响的，可以认为是“依次故障”，反之如果影响消除之前发生下一次故障，那么就可能丢数据，我们应该认为这种情况属于“同时故障”。
搞明白这点后，我们可以引入一个新的变量$T_r$表示恢复故障所需要的时间。当一次故障发生之后，$T_r$时间内不发生下一次故障的概率是：
$$ P(Failure(T_r)=0)=e^{-N\lambda T_r} $$
显然，出现两节点同时故障的概率 = 1 - 每次故障后的Tr时间内都不发生下一次故障的概率，而时间 t 内发生故障次数的期望是 $N\lambda t$，因此，时间 t 内出现“两节点同时故障”的概率是：
$$ P(Failure2(t)&amp;gt;0)=1-(e^{-N\lambda T_r})^{N\lambda t} = 1-e^{-N^2\lambda ^2 T_r t} \tag{2} $$
蒙特卡罗方法实验 所谓蒙特卡罗方法，其实就是统计模拟的方法，通过生成随机数进行大量的重复模拟实验，最后在大数定律的作用下，实验结果均值会逼近于期望值。这个方法对程序员来说是特别友好的，接下来我们就尝试用这个方法验证一下上文的概率推导。
为了先大概看一下蒙特卡罗方法是如何工作的，我先写了个简单的框架并验证了几何分布的期望值。代码很简单，模拟逻辑部分包含一个 estimated 表示预估值以及sim()函数运行一次模拟并返回实验结果，main函数就是不断地运行sim()并比较模拟平均值和预估值的差距。下面是一个简单的验证几何分布的例子：
运行结果精简之后如下，能观察到均值很快收敛在 0.5 附近，误差不超过 0.1%。
接下来我们考虑如何用蒙特卡罗方法来模拟服务器故障，思路也比较简单，就是把较长的时间分割成很多段较小的时间$\Delta t$，根据公式（1）可以算出$\Delta t$内每台服务器出现故障的概率，然后就用 for 循环 tick，不断判断每台服务器每段$\Delta t$是否故障。</description></item><item><title>偶数节点 raft</title><link>http://disksing.com/even-node-raft/</link><pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/even-node-raft/</guid><description>对 raft 有所了解的同学都知道，raft 一般会使用奇数个节点，比如 3，5，7 等等。这是因为 raft 是 一种基于多节点投票选举机制的共识算法，通俗地说，只有超过半数节点在线才能提供服务。这里超过半数的意思是N/2+1（而不是N/2），举例来说，3 节点集群需要 2 个以上节点在线，5 节点集群需要 3 个以上节点在线，等等。对于偶数节点的集群，2 节点集群需要 2 节点同时在线，4 节点集群需要 3 节点在线，以此类推。实际上不只是 raft，所有基于 Quorum 的共识算法大体上都是这么个情况，例如 Paxos，ZooKeeper 什么的，本文仅以 raft 为例讨论。
先考察一下为什么 raft 通常推荐使用奇数节点而不是偶数节点。
共识算法要解决的核心问题是什么呢？是分布式系统中单个节点的不可靠造成的不可用或者数据丢失。raft 保存数据冗余副本来解决这两个问题，当少数节点发生故障时，剩余的节点会自动重新进行 leader 选举（如果需要）并继续提供服务，而且 log replication 流程也保证了剩下的节点（构成 Quorum）总是包含了故障前成功写入的最新数据，因此也不会发生数据丢失。
我们对比一下 3 节点的集群和 4 节点的集群，Quorum 分别是 2 和 3，它们能容忍的故障节点数都是 1。如果深究的话，从概率上来说 4 节点集群发生 2 节点同时故障的可能性要更高一些。于是我们发现，相对于 3 节点集群，4 节点集群消耗更多的硬件资源，却换来了更差的可用性，显然不是个好选择。
但是！！！
上面说了，raft 解决的核心问题有两个，分别是高可用和数据容灾。跟奇数节点相比，偶数节点的方案从可用性上看很不划算，但是数据容灾方面却是有优势的。还是以 4 节点为例，因为 Quorum 是 3，写入数据的时候需要复制到至少 3 个节点才算写入成功，假如此时有 2 个节点同时故障，这种情况下虽然不可用了，但是剩余的两个节点一定包含有最新的数据，因此没有发生数据丢失。这一点很容易被忽视，在常见的奇数节点配置下，保证可用和保证数据不丢所容忍的故障节点数是重合的，但是在偶数节点配置下是不一样的。
根据上面的分析，偶数节点集群的适用场景是“能容忍一定时间的不可用，但不能容忍数据丢失”，应该有不少严肃的金融场景是符合这个描述的，毕竟一段时间不服务也比丢掉数据要强呀。
下面以两数据中心环境为例来对比一下。限制条件是任意一个数据中心故障时（比如发生严重自然灾害），能容忍一定时间的不可用，但不允许发生数据丢失。</description></item><item><title>histogram_quantile 相关的若干问题</title><link>http://disksing.com/histogram-quantile/</link><pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/histogram-quantile/</guid><description>histogram_quantile 是 Prometheus 特别常用的一个函数，比如经常把某个服务的 P99 响应时间来衡量服务质量。不过它到底是什么意思很难解释得清，特别是面向非技术的同学。另一方面，即使是资深的研发同学，在排查问题的时候也经常会发现 histogram_quantile 的数值出现一些反直觉的“异常现象”然后摸不着头脑。本文将结合原理和一些案例来分析这个问题。
统计学含义 Quantile 在统计学里面中文叫分位数，其中 X 分位数就是指用 X-1 个分割点把概率分布划分成 X 个具有相同概率的连续区间。常用的比如有二分位数，就是把数据分成两个等量的区间，这其实就是中位数了。还有当 X=100 时也叫百分位数（percentile），比如我们常说 P95 响应延迟是 100ms，实际上是指对于收集到的所有响应延迟，有 5% 的请求大于 100ms，95% 的请求小于 100ms。
Prometheus 里面的 histogram_quantile 函数接收的是 0-1 之间的小数，将这个小数乘以 100 就能很容易得到对应的百分位数，比如 0.95 就对应着 P95，而且还可以高于百分位数的精度，比如 0.9999。
quantile 的“反直觉案例” 问题1：P99 可能比平均值小吗？
正如中位数可能比平均数大也可能比平均数小，P99 比平均值小也是完全有可能的。通常情况下 P99 几乎总是比平均值要大的，但是如果数据分布比较极端，最大的 1% 可能大得离谱从而拉高了平均值。一种可能的例子：
1, 1, ... 1, 901 // 共 100 条数据，平均值=10，P99=1 问题2：服务 X 由顺序的 A，B 两个步骤完成，其中 X 的 P99 耗时 100ms，A 过程 P99 耗时 50ms，那么推测 B 过程的 P99 耗时情况是？</description></item></channel></rss>