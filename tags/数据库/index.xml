<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>数据库 on 硬盘在歌唱</title><link>http://disksing.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/</link><description>Recent content in 数据库 on 硬盘在歌唱</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 26 May 2020 00:00:00 +0000</lastBuildDate><atom:link href="http://disksing.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/index.xml" rel="self" type="application/rss+xml"/><item><title>数据库的外部一致性</title><link>http://disksing.com/external-consistency/</link><pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/external-consistency/</guid><description>一言以蔽之，外部一致性就是事务在数据库内的执行序列不能违背外部观察到的顺序。
举个例子，客户端先创建一个事务写入一条数据，然后再创建一个事务读取刚才写入的数据，这时候理应能读到，数据库不能给返回个空，然后解释说我把读事务安排在写事务前面了，所以啥也没读到。
传统上数据库的 ACID 里面是没有这个概念的，即使是最高级别的可串行化也没有外部一致性的约束——它只规定了多个事务的运行结果跟一个一个依次运行的结果相同，但没有关于事务次序的规定。也就是说，理论上，对于所有的只读事务，数据库都可以直接返回空，然后解释说所有的只读事务都排在第一条写事务的前面。当然了，没有数据库真的会这么干。正常的数据库都会按照事务请求到达的次序来执行，这不仅符合正常业务的需求，而且在实现层面也容易做。
当数据库进入分布式时代之后，外部一致性这个问题才真正需要被考虑，外部一致性这个概念本身也是 Google Spanner 论文里最早提出来的。原因是在分布式数据通常会保存同一份数据的多份副本来保证高可用和容灾，当多个副本所在的多个节点同时提供服务时，我们需要应付副本同步所带来的复杂性。
比如事务在一个节点写入一条数据，完成后立即另启一个事务在另一个节点读取，能成功读到刚刚写入的数据吗？如果没读到，可以理解成在数据库层面，后一个事务先于前一个事务运行了，这样就违背了外部所观察到的顺序。
再举一个涉及 3 个事务的例子。第一个事务在一个节点写入数据 A，完成后再启动第二个事务在另一个节点写入数据 B，在这个过程中另有第三个并发运行的事务尝试读 A 和 B，如果它读到了 B 却没读到 A，那意味着第二个事务先于第一个事务运行了，也不满足外部一致性。
要注意的是，外部观察到两个事务有先后次序，一定是前一个事务完成后，后一个事务才开始。否则两个事务是并发的，数据库可以以任意顺序执行这两个事务。例如客户端先启动一个事务，在请求发往数据库之后再启动第二个事务，这时这两个事务就是并发的。即使是单机数据库，也不能保证第一个事务的请求会先于第二个事务到达服务器，这两个事务的执行顺序完全有可能调换。
有朋友可能注意到外部一致性跟分布式系统里的线性一致性很类似。没错其实是本质上是一回事，不过线性一致性一般针对单个 key 的场景，外部一致性更侧重于对比传统数据库系统的内部一致性（即事务的时序在数据库系统内部是自洽的）。
TiDB 的外部一致性 TiDB 的方法是非常简单粗暴的，所有事务的 ts （用于标识事务的顺序）都要从中心节点 PD 获取，PD 在分配时保证 ts 严格单调递增。
因为所有事务都要通过同一个 PD 取 ts，假如在外部观察到两个事务有先后次序（如前所说，前一个事务提交完成后，第二个才启动），那么后面事务的 ts 一定会更大。于是，我们用 ts 的大小来规定事务的顺序，一定不会违背系统外部观察到的现象。
实际情况还要更复杂一点，因为事务往往涉及到多个节点，还需要使用 2PC 才能真正保证一致性，这里不展开了。
Spanner 的外部一致性 Spanner 最广为人知的就是它使用了原子钟进行授时。但实际上原子钟只是手段，真正有开创意义的是 TrueTime。利用各种硬件设备（大多数情况下主要起作用的其实是 GPS）和算法，TrueTime API 可以对外返回当前估算时间及误差范围，事务逻辑在考虑到误差之后进行一些补偿，最后就能实现外部一致性了。
原理其实比较简单，打个比方说明下：你跟妹子约会，商量好了 12 点整在电影院门口碰头，结果你等到 12 点还没见到人。这时候你是不会直接离开的，因为你会想可能是两人表的时间没对准，在妹子看来还没到 12 点。然后你一直等到 12:30，发现妹子还没来，你就知道自己是被放鸽子了，毕竟表不准也不太可能差这么多。更进一步，假如你能精确知道误差范围，譬如说误差不超过 10 分钟，那么你等到 12:10 就能知道肯定等不来人了。
Spanner 的事务正是这么做的，核心点就是事务提交的时候，会等待误差范围那么长的时间，然后才给客户端返回。这样一来，客户端接着再启动事务，或者客户端用某种方式通知另一个机房的客户启动事务（即使使用量子通信），新事务的取到的时间一定会比前面那个事务提交的时间要晚。
CockroachDB 的外部一致性 CockroachDB 用的是 HLC（混合逻辑时钟），使用的是结合物理时钟和逻辑时钟的时间戳。这个其实是权衡之后的方案：主打场景是类似 Spanner 那样的全球化部署，但是作为开源方案，也不可能用专有设备来搞一套 TrueTime。如果退而求其次用 NTP 的话，时钟误差无法控制下来会很大程度上影响性能。</description></item><item><title>分布式事务的 Commit Point</title><link>http://disksing.com/txn-commit-point/</link><pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate><guid>http://disksing.com/txn-commit-point/</guid><description>要说这个数据库事务啊，讲究的是 ACID。在分布式场景下，这四个没有一个是简单的，今天我们的话题主要涉及到 A(tomic)。
一、分布式环境的复杂性 在单机环境下，实现事务原子性并不复杂。一般的做法是事务提交之前的写入被存放在 预写式日志 中，然后在事务提交时，往磁盘追加一条 提交记录，完成事务的提交。
所谓 Commit Point，在这个场景下指的是 提交记录 被持久化到磁盘的一瞬间。在此之前，整个事务的写入都是未生效的状态，事务提交可能被回滚或中止（即使客户端已经发送了 Commit 命令，数据库可能在 Commit Point 之前崩溃）；而在 Commit Point 之后，整个事务就被提交成功了（即使由于数据库崩溃没来得及把结果返回给客户端）。
本质上，Commit Point 通过把事务内的多条 SQL 语句或者说多个对象的更新是否被提交“归约”到一个单点，也就是事务的 提交记录，从而确保了“要么同时提交，要么同时回滚”。
在分布式事务中，一个事务会同时牵扯到多个节点。这可能是因为事务本身要更新保存在不同节点的多个对象，也可能因为数据和索引保存在不同的节点（Global Index）。如果沿用单机数据库的经验，通过存储引擎中的 提交记录 来照葫芦画瓢，很容易出现原子性被破坏的情况：
部分节点成功提交，而部分节点由于冲突等原因需要回滚 部分节点成功提交，部分节点由于网络中断或崩溃无法提交 客户端与部分节点网络中断 客户端在向部分节点发送请求后崩溃 二、两阶段提交 两阶段提交（2PC）引入了 协调者（coordinator） 的角色，它通常以库的形式嵌入在发起事务的进程中，也可以以单独的进程或服务存在——这种情况下通常被称为 事务管理器（transaction manager）。同时我们把持有数据的存储节点称为 参与者（participant）。
两阶段提交（来源：设计数据密集型应用）
所谓两阶段，是 协调者 在提交的过程中，分两个步骤分别与 参与者 交互：
发送 准备（prepare） 请求给所有 参与者 ，询问是否可能提交。 如果所有 参与者 都回复YES，则发起第 2 阶段的 提交（commit） 真正提交；如果任意一个 参与者 回复NO或者超时无响应，则第 2 阶段改为 中止（abort） 回滚之前的操作。 这个过程类似西方婚礼时的流程。神父在第一阶段询问新娘和新郎是否要结婚，如果新娘和新郎都回复YES，神父才进入第二阶段，宣布二人结为夫妻。如果任意一个人说NO，结婚就中止了。
很显然，这个流程能防止“部分节点由于冲突等原因需要回滚”，但是并不能防住由于崩溃或者网络中断导致的不同步。好比神父在宣布二人结为夫妻时，新郎由于心情激动晕倒了没有听到，他醒来以后结婚还是未完成的状态，原子性就这样被破坏了。
解决这种“不同步”的关键正是 Commit Point，正如前面所说，我们需要把多个对象的更新归约到一个单点。实践中，Commit Point 可以有多种选择，下面我们来逐一分析：</description></item><item><title>理解 Google F1: Schema 变更算法</title><link>http://disksing.com/understanding-f1-schema-change/</link><pubDate>Wed, 18 Nov 2015 00:00:00 +0000</pubDate><guid>http://disksing.com/understanding-f1-schema-change/</guid><description>注：本文已被《从零开始写分布式数据库》一书收录。
背景 F1 是 Google 开发的分布式关系数据库，主要服务于 Google 的广告系统，它提供强一致性、高可用性，并支持传统 SQL 查询，近来也常常被称之为所谓的 NewSQL。
F1 是构建于 Spanner 之上的。Spanner 是 Google 开发的全球级数据存储引擎，它保证了数据存储的一致性和可用性，还通过 2PC（两阶段提交）提供了分布式事务读写。在分析 F1 时，我们可以简单地认为 Spanner 是一个全球分布的 kv 数据库。
F1 系统运行时由多台独立的 F1 服务器组成，为了保证整个系统的高可用性，F1 服务器被设计为无状态的，而且不存储数据——节点可以随时上线下线，客户端可以连接至任意节点发送请求。F1 服务器主要职能是将 RDBMS 中的结构化数据映射为可存储于 Spanner 的 kv 对，同时将客户端的 SQL 请求翻译成 get, set, del 等简单的 kv 操作。
Schema 也就是关系数据库中表、列、索引、约束等定义，对应于 SQL 中的 DDL。很显然，Schema 决定了 F1 服务器的具体工作方式，客户端请求的解析和验证由 Schema 决定，之后如何翻译成 kv 操作也由 Schema 决定，Schema 可以被认为是 F1 服务器运行时所依赖的元信息。实践中，F1 服务器运行时自身会缓存一份 Schema 并有一定的机制保持定时更新。
F1 中的 Schema 变更是在线的、异步的，Schema 变更的过程中所有数据保持可用，保持数据一致性，并最大限度的减小对性能的影响。最大的难点在于所有 F1 服务器的 Schema 变更是无法同步的，也就是说不同的 F1 服务器会在不同的时间点切换至新 Schema。</description></item></channel></rss>